{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacman with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brandon Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During my undergraduate degree, I was given the opportunity to work on a group project of my choice in a Software Engineering course. As most college students, we were interested in games and wanted to make something that would require a team effort. Immediately, I began thinking about recreating arcade games and suggested that we should do Pacman. The reason for this was simple, the game was easy enough for our group to complete on time, and I felt we could easily split the labor into making pacman, making the maze, and making the ghosts. Iknew that our project could (and would) be a great successs, but I did not know that this would start my obsession with artificial intelligence at the time.\n",
    "\n",
    "Quickly, I became obsessed with how sophisticated the ghost movement was in the game. For something that was developed in the 1980s, there was an immense complexity to how the ghosts behaved. Even though the algorithms behind them were simple, I could distinguish between each ghosts movements and behavior. Each had its own personality and watching them come to life for the first time was one of the most satisfying moments in my programming career. With this said, I have now had a crash course in far more advanced artificial intelligence and would like to revisit the beginning of my journey. In order to do this, I thought I should focus on the other side of the game, Pacman.\n",
    "\n",
    "Pacman is probably the perfect game for artificial intelligence. It has a finite world (given there are several states), the other agents that are involved have a pattern that can be learned, and it has a finite set of inputs. With this, I knew that I could solve this problem using reinforcement learning. I would able to construct keys for a Q dictionary with the state of the board and chose the best move for pacman accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I chose to use the pygame library to create a GUI. pygame is available for free down at https://www.pygame.org/news\n",
    "\n",
    "I used this reference for my initial project in Java. It breaks down exactly what each ghost is \"thinking\" http://gameinternals.com/post/2072558330/understanding-pac-man-ghost-behavior\n",
    "\n",
    "I had planned on doing all of the ghosts and power pellets in a full game but quickly realized that this may not be doable with my current understanding of Q Learning and my lack of resources for running the millions of tests required to have a properly trained algorithm. I have seen this done before on YouTube, but I believe that it would require a highly optimized and non generic algorithm. My purpose here is to simply modify Dr. Anderson's implementation of Q Learning with Tic Tac Toe to a more complex game. This will open up a number of possibilities for extending the algorithm to more and more complex games and will hopefully further help me dive into the depths of artificial intelligence and game design. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent the maze for the game, I chose to give each tile a key:\n",
    "\n",
    "\n",
    "Pac Man      = *\n",
    "\n",
    "Wall         = -\n",
    "\n",
    "Dot          = .\n",
    "\n",
    "Power Pellet = $\n",
    "\n",
    "Empty Tile   = _\n",
    "\n",
    "Warp Left    = L\n",
    "\n",
    "Warp Right   = R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to represent the initial maze in a text file. After doing some quick analysis we have these totals:\n",
    "868 tiles\n",
    "\n",
    "568 walls\n",
    "\n",
    "240 dots\n",
    "\n",
    "52 empty tiles\n",
    "\n",
    "4 power pellets\n",
    "\n",
    "4 ghosts\n",
    "\n",
    "2 warps\n",
    "\n",
    "and\n",
    "\n",
    "1 pac man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This gives us 240 + 52 + 4 + 2 + 1 + 1 = 300 walkable areas in the game. Note that only 1 ghost tile is accessible to Pac Man so I am only counting where Blinky starts at. Since each dot can turn into an empty tile that gives us * 240 = 72,000 + the possibility that each ghost and pac man can be in any of the 300 walkable areas as well. This quickly explodes to an unmanageable number of states. Because of this, I have to cut the number of states down drastically. I chose to make a smaller board 16x17, to cut the number of ghosts down to one (Blinky), and to remove all of the warps and power pellets. If I could make this work adequately then I would add the other factors in later or potentially just save this for another experiment on another day. Additionally, for state reduction I began thinking of what possible representations of the board would be valid. There where several failed version of my dictionary keys before I setting on the current iteration. Here is a rundown:\n",
    "\n",
    "For my Q scoring system I decided to check a couple things. I wanted Pacman to obviously go after pellets so I gave him a +1 reinforcement for every pellet he would consume. Obviously, if he cleared the board I wanted to make that extremely beneficial so I gave him +100 for clearing all pellets and winning the game. I decided to give him the complete opposite if he died so I gave him -100 for dying. This is something that would be revised several times during the experiment.\n",
    "\n",
    "\n",
    "My first attempt was simple enough: I was going to represent the state of the board as everything in the maze + the move. This was an astronomical disaster. The reason for this is that every single changing object (like a pellet being ate) made the board an entirely different state. This lead to an explosion of states that eventually lead to me receiving this very unfortunate feedback from Jupyter Notebook:\n",
    "\n",
    "[W 03:20:07.132 NotebookApp] IOPub data rate exceeded.                                                                      The notebook server will temporarily stop sending output                                                                to the client in order to avoid crashing it.                                                                            To change this limit, set the config variable                                                                           `--NotebookApp.iopub_data_rate_limit`.   \n",
    "\n",
    "I had too many states which was causing an explosion of memory usage so I decided that I needed to cutdown on the amount of states by changing what factors I considered. This lead to my second iteration.\n",
    "\n",
    "For my second attempt at representing the board, I decided to only track three variables: Pacman's position, Blinky's position and the move that was taken on that turn. This also proved to be unsuccessful because the variation still occurred too rapidly. Blinky could be in any of the available states as could Pacman, so Pacman still had no idea of what the best approach was and he would go to a corner and keep turning back and forth in the corner until Blinky came and killed him.\n",
    "\n",
    "The reason for this failure was I needed to adjust the scoring part of my Q learning. I decided to make several adjustments here. First, I chose add a decay for every move that Pacman did not eat a pellet. This is to discourage moving to tiles that have no benefit. I tweaked this a few times from a range of -0.00001 to -1 and eventually landed on -0.1. I also decided to change the reward for a eating a pellet. At first this was 3, but I felt that 1 would be easier to comprehend. Then, I made winning worth +50 and losing worth -50. \n",
    "\n",
    "With this new idea on what I should be scoring, I decided to revamp my state representation once again. At this iteration, I chose to make Pacman now consider his position, Blinky's position and the pellets consumed. Obviously this result was no better than the previously ones. Adding more things to the state is always a bad idea. The things that I was adding were also already part of the solution. Pacman already knows that Blinky is bad because he gets a negative reinforcement when he runs into him. Blinky's algorithm isn't random so Pacman does not need to explicitly know where Blinky is. The pellets are static, they are either in their location or already eaten, so Pacman does not need to actively know where they are for him to eat them. When he eats one he will know because of the positive reinforcement, and if he goes to a path that does not benefit him, he will also get a negative reinforcement.\n",
    "\n",
    "Next I tried to use the moves that took place. I kept adding the moves at the previous state to a list and using that as the dictionary key. This approached was flawed from the start as Pacman could take so many turns that the number of states was infinite. I was at a loss for what to do at this point so I scrapped my previous ideas and tried to get back to the basics of what makes Q learning successful. The concepts of Q learning are simple: you have an agent (Pacman) who has a goal (eat all of the pellets). With this idea, I decided that the most finite set for my states was the set of positions for Pacman + the set of positions for Blinky + the current move. This gave me 16X17X16X17X4 = 295936 states. Obviously this is still an enormous state space, but it was no longer infinite. At this point I was ready to begin testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Program setup\n",
    "\n",
    "My experiment was simple. I wrote the game of Pacman relatively quickly. Some of the more important methods that I defined were:\n",
    "\n",
    "### Helper functions\n",
    "   - `printMaze(maze)`:  \n",
    "   \n",
    "   \n",
    "   This function simply takes in a maze (numpy array from a text file) and prints it to the screen\n",
    " \n",
    " \n",
    "   - `noWall(direct,maze,sprite)`:\n",
    "   \n",
    "   \n",
    "    This function takes a direction, maze, and a sprite and determines if the sprite is going to collide with a wall if it moves in the direction in the maze.\n",
    "  \n",
    "   - `getOppositeDir(direct)`:\n",
    "  \n",
    "   returns the opposite direction of the specified direction\n",
    "  \n",
    "  \n",
    "   - `getValidMoves(sprite, maze)`:\n",
    "   \n",
    "   returns a list of valid moves for a sprite within its current position in the maze by checking that the directions do not intersect with a wall\n",
    "           \n",
    "    \n",
    "  - `initMaze(mazename, size, smartghosts,numghosts,ghostspositions)`:\n",
    "   \n",
    "  creates and returns pacman, ghosts, maze, background, displayheight, displaywidth. This requires the name of the file of the maze image and txt file to be the same (minus the extension). size is used for spacing between tiles, smartghosts is a parameter to determine what move function that the ghosts will use. numghosts is the number of ghosts to spawn in the game. ghostspositions are the positions that the ghosts will start at.\n",
    "  \n",
    "  \n",
    "  - `showMaze(maze, screen, size)`:\n",
    "   \n",
    "  this function is used to display the maze in pygame. It will print each of the pellets.\n",
    "           \n",
    "\n",
    "  - `hasCollision(sprite,ghost)`:\n",
    "   \n",
    "  this functions checks if the sprite (Pacman) collides with a ghost\n",
    "\n",
    "\n",
    "  - `updateMap(sprite, maze, ghosts)`:\n",
    "   \n",
    "  this functions updates the maze array with the new position of Pacman. It will also remove any of the pellets that Pacman has consumed and checks the status of the game\n",
    "\n",
    "\n",
    "  - `checkMove(sprite,maze,ghosts)`:\n",
    "       \n",
    "  this function is used by the Q Learning algorithm to check what the benefit of performing a move was.\n",
    "\n",
    "\n",
    "  - `printQ(Q)`:\n",
    "       \n",
    "  this functions prints the keys and values in the given Q dictionary\n",
    " \n",
    " \n",
    "  - `stateMoveTuple(pacman,ghosts,move)`:\n",
    "  \n",
    "  this function creates a tuple for the key in the Q dictionary by combining Pacman's position, the ghosts' position and the current move\n",
    "  \n",
    "  \n",
    "  - `playGame(Q,epsilonDecayFactor,learningRate,mazename,gamespeed,numGames,debug,graphics,smartghosts,numghosts,gpositions)`:\n",
    "       \n",
    "  this function plays the game.\n",
    "  \n",
    "   - Q: the Q dictionary to use\n",
    "  \n",
    "   - epsilonDecayFactor: the amount epsilon decays between games\n",
    "   \n",
    "   - learningRate: the amount of learning that occurs with each move\n",
    "   \n",
    "   - mazename: name of the maze file\n",
    "   \n",
    "   - gamespeed: speed that the graphical version of the game plays\n",
    "   \n",
    "   - numGames: number of games to play\n",
    "   \n",
    "   - debug: Boolean for printing debugging info\n",
    "   \n",
    "   - graphics: Boolean to determine whether to play the game in the background or with pygame\n",
    "   \n",
    "   - smartghosts: Boolean to determine ghost move function\n",
    "   \n",
    "   - numghosts: number of ghosts to spawn in the map\n",
    "   \n",
    "   - gpositions: starting ghost positions\n",
    "           \n",
    "### Move functions\n",
    "\n",
    "Below are the move functions that have been implemented in the game. In an early iteration of my experiment I included a user move function, but removed it because I didn't want to spend time for the experiment debugging it.\n",
    "\n",
    "  - `randomMove(sprite,maze)`:\n",
    "   \n",
    "  takes a random move from the valid moves for the sprite in the maze\n",
    "  \n",
    "  - `blinkyMove(sprite,maze)`:\n",
    " \n",
    "  creates a list of moves to pass to `getBlinkyMoves`. This functions tries to avoid moving back and forth when possible\n",
    "  \n",
    "  - `getBlinkyMove(sprite,options,targX,targY,maze)`:\n",
    "  \n",
    "  takes a list of options and checks it's position against its target's position. Then determines which of the options will move closest to the target in a straight line.\n",
    "  \n",
    "  \n",
    "  - `QMove(sprite, maze, Q, epsilon, ghosts)`:\n",
    "   \n",
    "  Q learning movement. This checks the Q dictionary for moves if a random number is less than the current epsilon. If the epsilon check passes, this function will call `getValidMoves` to get each possible move for the sprite, and then it will check the Q dictionary for these moves and returns the one with greatest value. If the epsilon check fails, it will return a random valid move.\n",
    "\n",
    "### Moving Object Class\n",
    "\n",
    "I implemented a class for this game. `MovingObject` represents each entity in the game that moves (Pacman and the ghosts).\n",
    "\n",
    "This class has several methods and properties.\n",
    "\n",
    "#### Methods\n",
    "\n",
    "* `getScreenPosition(self)`:\n",
    "    gets the objects position on the screen\n",
    "    \n",
    "* `getPosition(self)`:\n",
    "    gets position in the maze\n",
    "\n",
    "* `setPosition(self,x,y)`:\n",
    "    sets the objects position in the maze\n",
    "    \n",
    "* `getImages(self)`: \n",
    "    returns list of images for the object\n",
    "    \n",
    "    \n",
    "* `getImage(self,index)`\n",
    "     returns the current image for the object\n",
    "    \n",
    "* `getAnimPos(self)`:\n",
    "     returns the current frame of the object\n",
    "    \n",
    "* `updatePos(self)`:\n",
    "     updates the objects position          \n",
    "    \n",
    "* `updateAnimPos(self,direct,maze)`:\n",
    "     updates the objects animation frame\n",
    "        \n",
    "* `updatePrevPos(self)`:\n",
    "     updates the objects previous position\n",
    "\n",
    "* `getDir(self)`:\n",
    "     returns the objects current direction\n",
    "    \n",
    "* `setDir(self,direction)`:\n",
    "     sets the objects direction\n",
    "        \n",
    "* `move(self,maze, Q=None, epsilon=None, ghosts=None)`:\n",
    "     checks if Q and epsilon were passed. If so, calls QMove. Otherwise, calls objects move function\n",
    "        \n",
    "* `setSpaceSymbol(self, maze)`:\n",
    "     sets the space symbol\n",
    "        \n",
    "* `getScore(self)`:\n",
    "     returns the objects score (Not in use)\n",
    "    \n",
    "* `setScore(self, score)`:\n",
    "     sets the objects score (Not in use)\n",
    "        \n",
    "* `getSpaceSymbol(self)`:\n",
    "     gets the current space symbol\n",
    "    \n",
    "* `getSymbol(self)`:\n",
    "     returns the symbol for the object\n",
    "\n",
    "#### Properties\n",
    "\n",
    "* posX: x position in maze\n",
    "* posY: y position in maze\n",
    "* images: images for graphics\n",
    "* anims: animation frames for images\n",
    "* animPos: current animation frames\n",
    "* framesBetween: frames between updating position in maze\n",
    "* symbol: symbol in maze\n",
    "* spacing: offset spacing in maze\n",
    "* direction: direction that the object is moving\n",
    "* spacesymbol: symbol that is underneath the object in the maze\n",
    "* size: size of the object\n",
    "* prevX: previous X position in maze, used to update previous space symbol\n",
    "* prevY: previous Y position in maze, used to update previous space symbol\n",
    "* score: current score (not implemented)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "\n",
    "#### Learning Rate\n",
    "\n",
    "In my experiment, Pacman's environment is completely observable. You can see every pellet, ghost and Pacman himself. Because of this, a high learning rate would be ideal. If I had added in Clyde, who is completely random in his movements, I would have used a lower learning rate because the randomness would lead to more of a stochastic environment. Since the environment is deterministic, I settled on 0.95 as a learning rate. This is because each move that Pacman does in relation to Blinky should be the same. If Blinky is in one position and Pacman is in another, Pacman should always chose to move to a position that does not allow Blinky to kill him or a position that wins the game.\n",
    "\n",
    "#### Epsilon Decay Factor\n",
    "\n",
    "This is the hardest part of the game. Because there are so many states, it becomes incredibly difficult to determine an appropriate decay factor for epsilon. If epsilon decays to quickly, Pacman is still essentially taking random moves because he has never encountered the moves that he is currently taking. If it decays to slowly the the randomness could negatively impact your overall Q value for a particular entry.I tried to make my decay factors work to where they would become zero as the game approaches its final iterations; however, I realized after running thousands and thousands of tests that it would be best to allow Pacman to play completely randomly for an extended period of time and then start to decay epsilon. Currently I am testing 200000 random iterations with no epsilon decay and then a decaying epsilon for another 50000 iterations. At this point, I will run the game with graphics to see what the results look like. In previous experiments, it seemed that Pacman did not learn quickly enough and would take the same losing moves over and over. I am hoping that more iterations will allow Pacman to see his environment more effectively.\n",
    "\n",
    "#### Number of Trials\n",
    "\n",
    "I have tested this for probably a 500000 trials now trying to figure out the exact parameters to use, but I have settled on about 200000-250000 completely random trials and then another 50000-100000 decaying trials. Then I will test my results with graphics and a few hundred trials. If this experiment does not yield the results I would like, I will attempt 500000 random trials and about 150000 decaying trials. \n",
    "\n",
    "#### Q Learning Algorithm\n",
    "\n",
    "my Q learning algorithm is based on Assignment 5 from CS440 and Dr. Anderson's notes on Q Learning for Tic Tac Toe.\n",
    "\n",
    "\n",
    "    get the current direction\n",
    "    if the sprite can change direction (it is in the center of a tile: 0 for an animation frame)\n",
    "        get a list of valid moves\n",
    "        if random is l than epsilon\n",
    "            get a random move\n",
    "            set direction to the random move\n",
    "        others\n",
    "            Get the list of Q dictionary values from the valid moves\n",
    "            set direction to the hightest Q value, if the value isn't in the list initial to 0\n",
    "    \n",
    "    if the current key is not in the Q dictionary add it\n",
    "        \n",
    "    update the sprites animations and movement\n",
    "    return the direction and Q\n",
    "    \n",
    "    add the value of the current move to the current entry in the Q dictionary\n",
    "    propogate this value back to the previous entry in the dictionary\n",
    "    update the previous key to the current key\n",
    "    \n",
    "    \n",
    "#### Results\n",
    "\n",
    "My first several iterations of this experiment were colossal failures. I was previously adding the ghosts (Blinky) to the maze and trying to replace his symbol with the symbol he had ran over on the previous move, but I had several issues getting this to function properly. I chose instead to no longer represent the ghosts in the maze, and instead just keep their positions with their objects. Additionally, I attempted to give Pacman positive reinforcement for eating individual pellets, negative reinforcement for moving to tiles that give him no benefit, positive reinforcement for winning, and negative reinforcement for being killed, but I found out that these variables could skew the results in longer games. If Pacman ate several pellets then the result could outweigh dying or he could chose not to take a more beneficial path because a pellet did not exist there. \n",
    "\n",
    "To compensate for this, I chose to simplify my approach as much as possible. Only two states gave Pacman a reinforcement: winning and losing. This approach allowed me to focus only on keeping Pacman alive long enough to hit eat every pellet. I assumed that if he become very good at avoiding Blinky, eventually he would hit all points of the maze and clear the pellets. I feel that it would be more optimal to have the pellets and time taken actually factor into the score, but with my time crunch, I was unable to find a solution that adequately solved the problem.\n",
    "\n",
    "I had the output of the non graphical playthroughs display at every 500 trials. This would display the number of games, number of wins, number of losses and current epsilon. This allowed me to see if I was making any progress or if I needed to make tweaks to the experiment and start over. Additionally, I printed the Q dictionary between each set of trials to determine if any extreme erroneous data had been accrued. In some early instance, I was getting values that were all 0's (not enough trials), negative infinity (over incrementing the Q values), and values in the 1000s (too many values being considered). These results kept me checking and tweaking my experiment very often.\n",
    "\n",
    "The most unusual results that I had were trials that had the results converging to lose every match. Ironically, the average amount of wins for the random agent was about 20-30 in 500 or 5%, but an insufficiently trained agent will win a grand total of 0% of the time. This was due to several factors, but I suspect that my Q learning algorithm had some errors because it was subtracting the value of previous negative states and making them positive. I attempted to correct this, and hopefully the results of my final set of trials will be more correct.\n",
    "\n",
    "\n",
    "For my results of 500,000 matches, it seemed that either the number of matches was insufficient to adequately represent the board, my key for the board was not precise enough to describe the situation, and/or my reinforcements were not precise enough to represent what is happening. I believe the reason for this is simple. Pacman needs to know more information that just his position and Blinky's position to be successful. If the only reinforcements Pacman is concerned with is staying away from Blinky then he is going to not be concerned with eating the pellets in the maze. If Pacman is concerned with eating pellets then he needs to know the locations of those pellets and whether or not they have been eaten. It makes no logical sense for Pacman to get a positive reinforcement for a key of P(12,1) B(12,2) L if he eats a pellet because the next time he comes to that spot, he will not receive the same positive reinforcement. This dilemma made this experiment go unsucessful to this point, and unfortunately the time constraints to run a million matches is making this experiment impossible to finish before the deadline. I attempted to use some shortcuts to make the results skewed to be much better. I modified the valid moves function to disallow moves in the opposite direction for random moves. This helped to win more random games and gave Pacman more of an idea of what was a good move to begin with. Unfortunately, I chose to allow those moves with the Q learning algorithm which causes Pacman to turn around abruptly at some points and die.\n",
    "\n",
    "For a last ditch effort prior to the deadline. I chose to run 100,000 matches with no opposite moves. I think this will at least promote a better solution than what had occurred previously. I also chose to tweak the learningRate to be .75. I had previously experiment with rates as low as .2 and as high as .9, but with the amount of time left to test, I feel this is my only chance to have the results work properly. Finally, for this iteration I chose to give the same benefit for clearing the board as eating a pellet 0.01. This means that the highest impact on Pacman is dying. There should be no random movements to a space with no benefit because Pacman should always be mindful of staying away from Blinky. The slight reinforcement for eating pellets should hopefully make Pacman want to move around the map instead of staying in one spot, and the disallowance of opposite moves will prevent wiggling back and forth.\n",
    "\n",
    "\n",
    "#### What I Would Like to Test Next\n",
    "\n",
    "I think that a neural network would be the best approach to solving this problem. You can represent all of the states without the massive amount of keys that would be required to represent every situation in Pacman. I also believe that I could potential look into splitting the map into several sectors and have the keys only represent a small area around Pacman. If I did this segment key it would look something like this\n",
    "\n",
    "    `P(X,Y) B(X,Y) PelletsByDirection Direction`\n",
    "    \n",
    "I think this result may be more optimal because it would prevent at least some of the state explosion that would occur with listing the entire maze, and it would allow Pacman to make more wise decisions from his current position because he would know what"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the numpy loadtxt function to load a text file maze.txt that represents the maze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, pygame\n",
    "from pygame.locals import *\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "useQ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printMaze(maze):\n",
    "    for i in range(len(maze)):\n",
    "        for j in range(len(maze[0])):\n",
    "            print(maze[i][j], end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - - - - - - - \n",
      "- . . . . . . - - . . . . . . - \n",
      "- . - - - - . - - . - - - - . - \n",
      "- . - - - - . - - . - - - - . - \n",
      "- . - - - - . - - . - - - - . - \n",
      "- . - - - - . - - . - - - - . - \n",
      "- . . . . . . . . . . . . . . - \n",
      "- . - - - _ - - - - _ - - - . - \n",
      "- . - - - _ - - - - _ - - - . - \n",
      "- . - - - _ - - - - _ - - - . - \n",
      "- . - - - _ - - - - _ - - - . - \n",
      "- . - - - _ - - - - _ - - - . - \n",
      "- . . . . . . . * . . . . . . - \n",
      "- . - - - - - - - - - - - - . - \n",
      "- . - - - - - - - - - - - - . - \n",
      "- . . . . . . . . . . . . . . - \n",
      "- - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "printMaze(np.genfromtxt(\"small-maze.txt\", dtype=\"str\", delimiter=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "- . . . . . . . . . . . . - - . . . . . . . . . . . . - \n",
      "- . - - - - . - - - - - . - - . - - - - - . - - - - . - \n",
      "- $ - - - - . - - - - - . - - . - - - - - . - - - - $ - \n",
      "- . - - - - . - - - - - . - - . - - - - - . - - - - . - \n",
      "- . . . . . . . . . . . . . . . . . . . . . . . . . . - \n",
      "- . - - - - . - - . - - - - - - - - . - - . - - - - . - \n",
      "- . - - - - . - - . - - - - - - - - . - - . - - - - . - \n",
      "- . . . . . . - - . . . . - - . . . . - - . . . . . . - \n",
      "- - - - - - . - - - - - _ - - _ - - - - - . - - - - - - \n",
      "- - - - - - . - - - - - _ - - _ - - - - - . - - - - - - \n",
      "- - - - - - . - - _ _ _ _ B _ _ _ _ _ - - . - - - - - - \n",
      "- - - - - - . - - _ - - - - - - - - _ - - . - - - - - - \n",
      "- - - - - - . - - _ - _ _ _ _ _ _ - _ - - . - - - - - - \n",
      "L _ _ _ _ _ . _ _ _ - I _ P _ _ C - _ _ _ . _ _ _ _ _ R \n",
      "- - - - - - . - - _ - _ _ _ _ _ _ - _ - - . - - - - - - \n",
      "- - - - - - . - - _ - - - - - - - - _ - - . - - - - - - \n",
      "- - - - - - . - - _ _ _ _ _ _ _ _ _ _ - - . - - - - - - \n",
      "- - - - - - . - - _ - - - - - - - - _ - - . - - - - - - \n",
      "- - - - - - . - - _ - - - - - - - - _ - - . - - - - - - \n",
      "- . . . . . . . . . . . . - - . . . . . . . . . . . . - \n",
      "- . - - - - . - - - - - . - - . - - - - - . - - - - . - \n",
      "- . - - - - . - - - - - . - - . - - - - - . - - - - . - \n",
      "- $ . . - - . . . . . . . _ * . . . . . . . - - . . $ - \n",
      "- - - . - - . - - . - - - - - - - - . - - . - - . - - - \n",
      "- - - . - - . - - . - - - - - - - - . - - . - - . - - - \n",
      "- . . . . . . - - . . . . - - . . . . - - . . . . . . - \n",
      "- . - - - - - - - - - - . - - . - - - - - - - - - - . - \n",
      "- . - - - - - - - - - - . - - . - - - - - - - - - - . - \n",
      "- . . . . . . . . . . . . . . . . . . . . . . . . . . - \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "printMaze(np.genfromtxt(\"maze.txt\", dtype=\"str\", delimiter=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MovingObject:\n",
    "    posX = 0\n",
    "    posY = 0\n",
    "    images = []\n",
    "    anims = {}\n",
    "    animPos = 0\n",
    "    framesBetween = 0\n",
    "    symbol = \"\"\n",
    "    spacing = 0\n",
    "    direction = \"\"\n",
    "    spacesymbol = \"_\"\n",
    "    size = 0\n",
    "    prevX = -1\n",
    "    prevY = -1\n",
    "    score = 0\n",
    "    \n",
    "    def __init__(self, images, x, y, moveF, anims, framesBetween, symbol, spacing, size, direction):\n",
    "        self.posX = x\n",
    "        self.posY = y\n",
    "        self.prevX = x\n",
    "        self.prevY = y\n",
    "        self.nextX = x\n",
    "        self.nextY = y\n",
    "        self.images = copy.copy(images)\n",
    "        self.moveF = moveF\n",
    "        self.anims = copy.deepcopy(anims)\n",
    "        self.framesBetween = framesBetween\n",
    "        self.symbol = symbol\n",
    "        self.spacing = spacing\n",
    "        self.size = size\n",
    "        self.direction = direction\n",
    "        \n",
    "    def getScreenPosition(self):\n",
    "        offsetX = -self.spacing\n",
    "        offsetY = -self.spacing * 2\n",
    "        if self.direction == 'L':\n",
    "            offsetX -= self.spacing * self.animPos\n",
    "        if self.direction == 'R':\n",
    "            offsetX += self.spacing * self.animPos\n",
    "        if self.direction == 'U':\n",
    "            offsetY -= self.spacing * self.animPos\n",
    "        if self.direction == 'D':\n",
    "            offsetY += self.spacing * self.animPos\n",
    "        return (self.posX*size+offsetX, self.posY*size+offsetY)\n",
    "    \n",
    "    def getPosition(self):\n",
    "        return self.posX, self.posY\n",
    "    \n",
    "    def setPosition(self,x,y):\n",
    "        self.posX = x\n",
    "        self.posY = y\n",
    "    \n",
    "    def getImages(self):\n",
    "        return self.images\n",
    "\n",
    "    def getImage(self,index):\n",
    "        if len(self.anims[self.direction]) > index:\n",
    "            return self.images[self.anims[self.direction][index]]\n",
    "        else:\n",
    "            return self.images[self.anims[self.direction][0]]\n",
    "    \n",
    "    def getAnimPos(self):\n",
    "        return self.animPos\n",
    "    \n",
    "    def updatePos(self):\n",
    "        if self.animPos == 0:\n",
    "            self.updatePrevPos()\n",
    "            if self.direction == 'L':\n",
    "                self.posX -= 1\n",
    "            if self.direction == 'R':\n",
    "                self.posX += 1\n",
    "            if self.direction == 'U':\n",
    "                self.posY -= 1\n",
    "            if self.direction == 'D':\n",
    "                self.posY += 1             \n",
    "    \n",
    "    def updateAnimPos(self,direct,maze):\n",
    "        if direct == self.direction:\n",
    "            self.animPos = (self.animPos + 1) % self.framesBetween\n",
    "        elif self.animPos == 0:\n",
    "            self.direction = direct\n",
    "        else:\n",
    "            if (direct == 'L' and self.direction == 'R' or direct == 'R' and self.direction == 'L' or\n",
    "                direct == 'U' and self.direction == 'D' or direct == 'D' and self.direction == 'U'):\n",
    "                self.animPos = (self.framesBetween - self.animPos) % self.framesBetween \n",
    "                self.direction = direct\n",
    "        self.updatePos()\n",
    "        \n",
    "    def updatePrevPos(self):\n",
    "        self.prevX = self.posX\n",
    "        self.prevY = self.posY\n",
    "\n",
    "    def getDir(self):\n",
    "        return self.direction\n",
    "    \n",
    "    def setDir(self,direction):\n",
    "        self.direction = direction\n",
    "        \n",
    "    def move(self,maze, Q=None, epsilon=None, ghosts=None):\n",
    "        if not useQ or not Q and not epsilon:\n",
    "            self.moveF(self,maze)\n",
    "        else:\n",
    "            return self.moveF(self,maze,Q,epsilon,ghosts)\n",
    "        \n",
    "    def setSpaceSymbol(self, maze):\n",
    "        self.spacesymbol = maze[self.posY][self.posX]\n",
    "        \n",
    "    def getScore(self):\n",
    "        return self.score\n",
    "    \n",
    "    def setScore(self, score):\n",
    "        self.score = score\n",
    "        \n",
    "    def getSpaceSymbol(self):\n",
    "        return self.spacesymbol\n",
    "    \n",
    "    def getSymbol(self):\n",
    "        return self.symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noWall(direct,maze,sprite):\n",
    "    if direct == 'L':\n",
    "        return (maze[sprite.posY][sprite.posX-1] != '-')\n",
    "    if direct == 'R':\n",
    "        return (maze[sprite.posY][sprite.posX+1] != '-')\n",
    "    if direct == 'U':\n",
    "        return (maze[sprite.posY-1][sprite.posX] != '-')\n",
    "    if direct == 'D':\n",
    "        return (maze[sprite.posY+1][sprite.posX] != '-')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOppositeDir(direct):\n",
    "    if direct == 'L':\n",
    "        return 'R'\n",
    "    elif direct == 'R':\n",
    "        return 'L'\n",
    "    elif direct == 'U':\n",
    "        return 'D'\n",
    "    elif direct == 'D':\n",
    "        return 'U'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getValidMoves(sprite, maze, allowOpposite):\n",
    "    dirs = [\"L\", \"R\", \"U\", \"D\"]\n",
    "    options = []\n",
    "    for d in dirs:\n",
    "        if allowOpposite:\n",
    "            if noWall(d,maze,sprite): \n",
    "                options.append(d)\n",
    "        else:\n",
    "            if noWall(d,maze,sprite) and getOppositeDir(sprite.getDir()) != d: \n",
    "                options.append(d)\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomMove(sprite,maze):\n",
    "    direct = sprite.getDir()\n",
    "    if sprite.getAnimPos() == 0:\n",
    "        options = getValidMoves(sprite, maze, False)\n",
    "        if len(options) > 0:\n",
    "            direct = options[random.randint(0,len(options)-1)]\n",
    "    if (noWall(direct,maze,sprite)):        \n",
    "        sprite.updateAnimPos(direct,maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBlinkyMove(sprite,options,targX,targY,maze):\n",
    "    bestOptions = []\n",
    "    if targX < sprite.getPosition()[0] and \"L\" in options:\n",
    "        bestOptions.append(\"L\")\n",
    "    if targX > sprite.getPosition()[0] and \"R\" in options:\n",
    "        bestOptions.append(\"R\")\n",
    "    if targY < sprite.getPosition()[1] and \"U\" in options:\n",
    "        bestOptions.append(\"U\")\n",
    "    if targY > sprite.getPosition()[1] and \"D\" in options:\n",
    "        bestOptions.append(\"D\")\n",
    "        \n",
    "    if len(bestOptions) > 0:\n",
    "        best = 0\n",
    "        dist = 10000000\n",
    "        for i in range(len(bestOptions)):\n",
    "            if bestOptions[i] == \"L\" and sprite.getPosition()[0] - targX < dist:\n",
    "                best = i\n",
    "                dist = sprite.getPosition()[0] - targX\n",
    "                if dist == 0:\n",
    "                    dist += abs(targY - sprite.getPosition()[1])\n",
    "            if bestOptions[i] == \"R\" and targX - sprite.getPosition()[0] < dist:\n",
    "                best = i\n",
    "                dist = targX - sprite.getPosition()[0]\n",
    "                if dist == 0:\n",
    "                    dist += abs(targY - sprite.getPosition()[1])\n",
    "            if bestOptions[i] == \"U\" and targY - sprite.getPosition()[1] < dist:\n",
    "                best = i\n",
    "                dist = targY - sprite.getPosition()[1]\n",
    "                if dist == 0:\n",
    "                    dist += abs(targX - sprite.getPosition()[0])\n",
    "            if bestOptions[i] == \"D\" and sprite.getPosition()[1] - targY < dist:\n",
    "                best = i\n",
    "                dist = sprite.getPosition()[1] - targY\n",
    "                if dist == 0:\n",
    "                    dist += abs(targX - sprite.getPosition()[0])\n",
    "            # discourage from going back and forth\n",
    "            if bestOptions[i] == getOppositeDir(sprite.getDir()):\n",
    "                dist += 10              \n",
    "        return bestOptions[best]\n",
    "    else:\n",
    "        return options[random.randint(0,len(options)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blinkyMove(sprite,maze):\n",
    "    direct = sprite.getDir()\n",
    "    if sprite.getAnimPos() == 0:\n",
    "        dirs = [\"L\", \"R\", \"U\", \"D\"]\n",
    "        # boolean to keep from abruptly turning\n",
    "        changeDir = 0\n",
    "        options = []\n",
    "        for d in dirs:\n",
    "            if noWall(d,maze,sprite): \n",
    "                options.append(d)\n",
    "                if d != direct and d != getOppositeDir(direct):\n",
    "                    changeDir = 1\n",
    "        if changeDir and '*' in maze:\n",
    "            pacmanpos = np.where(maze=='*')\n",
    "            pacmanx = pacmanpos[1][0]\n",
    "            pacmany = pacmanpos[0][0]\n",
    "            direct = getBlinkyMove(sprite,options, pacmanx,pacmany,maze)\n",
    "        else:\n",
    "            if not noWall(direct,maze,sprite):\n",
    "                direct = getOppositeDir(direct)\n",
    "    sprite.updateAnimPos(direct,maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a tuple with the first\n",
    "# element as the state and the\n",
    "# second as the move\n",
    "def stateMoveTuple(pacman,ghosts,move,symbol):\n",
    "    t = \"P\"+str(pacman.getPosition())\n",
    "    for g in ghosts:\n",
    "        t += \" \" + g.getSymbol() + str(g.getPosition())\n",
    "    t += \" \" + move\n",
    "    t += \" \" + symbol\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTile(sprite,maze,m):\n",
    "        if m == 'L':\n",
    "            return maze[sprite.posY][sprite.posX - 1]\n",
    "        if m == 'R':\n",
    "            return maze[sprite.posY][sprite.posX + 1]\n",
    "            self.posX += 1\n",
    "        if m == 'U':\n",
    "            return maze[sprite.posY - 1][sprite.posX]\n",
    "        if m == 'D':\n",
    "            return maze[sprite.pos + 1][sprite.posX]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def QMove(sprite, maze, Q, epsilon, ghosts):\n",
    "    direct = sprite.getDir()\n",
    "    if (sprite.getAnimPos() == 0):\n",
    "        if np.random.uniform() < epsilon:\n",
    "            validMoves = getValidMoves(sprite, maze,False)\n",
    "            index = random.randint(0,len(validMoves)-1)\n",
    "            direct = validMoves[index]\n",
    "        else:\n",
    "            validMoves = getValidMoves(sprite, maze,False)\n",
    "            Qs = np.array([Q.get(stateMoveTuple(sprite,ghosts,m, getTile(sprite,maze,m)), 0) for m in validMoves])\n",
    "            direct = validMoves[np.argmax(Qs)]\n",
    "    if stateMoveTuple(sprite, ghosts,direct,getTile(sprite,maze,direct)) not in Q:\n",
    "        Q[stateMoveTuple(sprite,ghosts,direct,getTile(sprite,maze,direct))] = 0\n",
    "    key = stateMoveTuple(sprite,ghosts,direct,getTile(sprite,maze,direct))\n",
    "    sprite.updateAnimPos(direct,maze)\n",
    "    \n",
    "    return Q, direct, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 12)\n"
     ]
    }
   ],
   "source": [
    "def initMaze(mazename,size, smartghosts,numghosts,ghostspositions):\n",
    "    maze = np.genfromtxt(mazename +\".txt\", dtype=\"str\", delimiter=\" \")\n",
    "    displaywidth = size * len(maze[0])\n",
    "    displayheight = size * len(maze)\n",
    "    background = pygame.image.load(mazename +\".png\")\n",
    "    pacmanimages = []\n",
    "    pacmanimages.append(\"pacman-closed.png\")\n",
    "    pacmanimages.append(\"pacman-open-down.png\")\n",
    "    pacmanimages.append(\"pacman-open-left.png\")\n",
    "    pacmanimages.append(\"pacman-open-right.png\")\n",
    "    pacmanimages.append(\"pacman-open-up.png\")\n",
    "    pacmanimages.append(\"pacman-wide-down.png\")\n",
    "    pacmanimages.append(\"pacman-wide-left.png\")\n",
    "    pacmanimages.append(\"pacman-wide-right.png\")\n",
    "    pacmanimages.append(\"pacman-wide-up.png\")\n",
    "    pacmananims = {}\n",
    "    pacmananims[\"D\"] = [0,1,5,1]\n",
    "    pacmananims[\"L\"] = [0,2,6,2]\n",
    "    pacmananims[\"R\"] = [0,3,7,3]\n",
    "    pacmananims[\"U\"] = [0,4,8,4]\n",
    "    pacmansymbol = '*'\n",
    "    pacmanpos = np.where(maze==pacmansymbol)\n",
    "    pacmanx = pacmanpos[1][0]\n",
    "    pacmany = pacmanpos[0][0]\n",
    "    if useQ:\n",
    "        moveF = QMove\n",
    "    else:\n",
    "        moveF = randomMove\n",
    "    pacman = MovingObject(pacmanimages, pacmanx, pacmany, moveF, pacmananims, len(pacmananims),\n",
    "          pacmansymbol, 4, size, 'L')\n",
    "    ghosts = []\n",
    "    ghostsymbols = ['B','P','I','C']\n",
    "    ghostnames = ['blinky','pinky', 'inky', 'clyde']\n",
    "    for i in range(numghosts):\n",
    "        ghostimages = [ghostnames[i]+\"-down.png\",ghostnames[i]+\"-left.png\",ghostnames[i]+\"-right.png\",\n",
    "            ghostnames[i]+\"-up.png\", \"frightened-blue.png\", \"frightened-white.png\"]\n",
    "        ghostanims = {}\n",
    "        ghostanims[\"D\"] = [0]\n",
    "        ghostanims[\"L\"] = [1]\n",
    "        ghostanims[\"R\"] = [2]\n",
    "        ghostanims[\"U\"] = [3]\n",
    "        ghostanims[\"F\"] = [4,5]\n",
    "        ghostpos = np.where(maze==ghostsymbols[i])\n",
    "        ghostx = ghostspositions[i][1]\n",
    "        ghosty = ghostspositions[i][0]\n",
    "        if smartghosts:\n",
    "            moveF = blinkyMove\n",
    "        else:\n",
    "            moveF = randomMove\n",
    "        ghosts.append(MovingObject(ghostimages, ghostx, ghosty, moveF,\n",
    "            ghostanims, 4, ghostsymbols[i], 4, size, 'L'))\n",
    "            \n",
    "    return maze, pacman, ghosts, background, displaywidth, displayheight\n",
    "\n",
    "maze, pacman, ghosts, background, displaywidth, displayheight = initMaze(\"small-maze\",24, True,1,[(6,7)])\n",
    "print(pacman.getPosition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showMaze(maze, screen, size):\n",
    "    for i in range(len(maze)):\n",
    "        for j in range(len(maze[i])):\n",
    "            if maze[i][j]=='.':    \n",
    "                screen.blit(pygame.image.load(\"pellet.png\"),(j*size+8,i*size+8)) \n",
    "            if maze[i][j]=='$':    \n",
    "                screen.blit(pygame.image.load(\"power-pellet.png\"),(j*size-4,i*size-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hasCollision(sprite,ghost):\n",
    "    if sprite.posX == ghost.posX and sprite.posY == ghost.posY:\n",
    "        return True\n",
    "    if sprite.getAnimPos() >= 2 and ghost.getAnimPos() >= 2 and getOppositeDir(sprite.getDir()) == ghost.getDir():\n",
    "        if sprite.getDir() == \"L\" and sprite.posY == ghost.posY and sprite.posX == ghost.posX - 1:\n",
    "            return True\n",
    "        if sprite.getDir() == \"R\" and sprite.posY == ghost.posY and sprite.posX - 1 == ghost.posX:\n",
    "            return True\n",
    "        if sprite.getDir() == \"U\" and sprite.posX == ghost.posX and sprite.posY - 1 == ghost.posY:\n",
    "            return True\n",
    "        if sprite.getDir() == \"D\" and sprite.posX == ghost.posX and sprite.posY == ghost.posY -1:\n",
    "            return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateMap(sprite, maze, ghosts):\n",
    "    result = 0\n",
    "    symbol = '_'\n",
    "    if sprite.prevX != -1 and sprite.prevY != -1: \n",
    "        if maze[sprite.prevY][sprite.prevX] == '.':\n",
    "            result += 0.01\n",
    "        maze[sprite.prevY][sprite.prevX] = sprite.getSpaceSymbol()\n",
    "    symbol = maze[sprite.posY][sprite.posX]\n",
    "    maze[sprite.posY][sprite.posX] = '*'\n",
    "    if not '.' in maze:\n",
    "        return True,0.01, symbol\n",
    "    for g in ghosts:\n",
    "        if hasCollision(sprite, g):\n",
    "            return True,-1, symbol\n",
    "    return False,result, symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkMove(sprite,maze,ghosts):\n",
    "    if not '.' in maze:\n",
    "        return 1\n",
    "    for g in ghosts:\n",
    "        if hasCollision(sprite,g):\n",
    "            return -1 \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countQZeros(Q):\n",
    "    count = 0\n",
    "    for i in Q:\n",
    "        if i == 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printNonZero(Q):\n",
    "    keys = list(Q)\n",
    "    for i in range(len(keys)):\n",
    "        if Q[keys[i]] != 0:\n",
    "            print(keys[i], Q[keys[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printQ(Q):\n",
    "    keys = list(Q)\n",
    "    for i in range(len(keys)):\n",
    "        print(keys[i], Q[keys[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Q = {}\n",
    "pygame.init()\n",
    "numGames = 10000\n",
    "epsilonDecayFactor = 0.95\n",
    "learningRate = 0.75\n",
    "mazename = \"small-maze\"\n",
    "# around 20 for normal speed, 100+ for training or turn graphics off\n",
    "gamespeed = 100\n",
    "debug = False\n",
    "size = 24\n",
    "smartghosts = True\n",
    "graphics = True\n",
    "numghosts = 1\n",
    "gpositions = [(6,7)]\n",
    "showRate = 10\n",
    "\n",
    "def playGame(Q,epsilonDecayFactor,learningRate,mazename,gamespeed,numGames,debug,graphics,smartghosts,numghosts,gpositions,showRate):\n",
    "    epsilon = 1.0\n",
    "    curgame = 1\n",
    "    numL = 0\n",
    "    numW = 0\n",
    "    for i in range(numGames):\n",
    "        totalScore = 0\n",
    "        moves = []\n",
    "        epsilon *= epsilonDecayFactor\n",
    "        if debug:\n",
    "            print(epsilon)\n",
    "        maze, pacman, ghosts, background, displaywidth, displayheight = initMaze(mazename,size,smartghosts,numghosts,gpositions)        \n",
    "        if graphics:\n",
    "            screen = pygame.display.set_mode((displaywidth, displayheight))\n",
    "            pygame.display.set_caption('Pacman! Game: ' + str(curgame))\n",
    "            clock = pygame.time.Clock()\n",
    "        gameOver = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        if debug:\n",
    "            printMaze(maze)\n",
    "        while not gameOver:           \n",
    "            if graphics:\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        gameOver = 1\n",
    "                    if event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_ESCAPE:\n",
    "                            pygame.quit()\n",
    "                            sys.exit\n",
    "                            gameOver = 1\n",
    "            if not gameOver:\n",
    "                score = pacman.getScore()\n",
    "                if graphics:\n",
    "                    pygame.display.set_caption('Pacman! Game: ' + str(curgame) + \" Epsilon: \" + str(epsilon))\n",
    "                    screen.blit(background,(0,0))\n",
    "                    \n",
    "                if useQ:\n",
    "                    Q, move, key = pacman.move(maze, Q, epsilon, ghosts)\n",
    "                else:\n",
    "                    pacman.move(maze)\n",
    "                [x.move(maze) for x in ghosts] \n",
    "                gameOver,result, symbol = updateMap(pacman, maze, ghosts)\n",
    "                if useQ:\n",
    "                    if result < 0:\n",
    "                        Q[key] += learningRate * (-1 - Q[key])\n",
    "                    else: \n",
    "                        Q[key] = result\n",
    "                        \n",
    "                    if step > 0 and stateMoveTuple(prevpacman,prevghosts,moveOld,prevSymbol) in Q.keys():\n",
    "                        Q[stateMoveTuple(prevpacman,prevghosts,moveOld, prevSymbol)] += learningRate * (Q[key] - Q[stateMoveTuple(prevpacman,prevghosts,moveOld,prevSymbol)])\n",
    "                    moveOld = move\n",
    "                    prevpacman = copy.deepcopy(pacman)\n",
    "                    prevghosts = copy.deepcopy(ghosts)\n",
    "                    prevSymbol = symbol\n",
    "                    step += 1\n",
    "                if gameOver and result > 0:\n",
    "                    numW += 1\n",
    "                elif result == -1:\n",
    "                    numL += 1\n",
    "                if graphics:\n",
    "                    showMaze(maze,screen,size)\n",
    "                    screen.blit(pygame.image.load(pacman.getImage(pacman.getAnimPos())),\n",
    "                        (pacman.getScreenPosition()))\n",
    "                    [screen.blit(pygame.image.load(x.getImage(x.getAnimPos())),(x.getScreenPosition())) for x in ghosts]\n",
    "                    pygame.display.flip()\n",
    "                    clock.tick(gamespeed)\n",
    "                if debug:\n",
    "                    print(pacman.posX,pacman.posY)\n",
    "                    print(pacman.prevX,pacman.prevY)\n",
    "                    printMaze(maze)\n",
    "                    print()\n",
    "        # print game number every showRate games\n",
    "        if not graphics and curgame % showRate == 0:\n",
    "            print(curgame,numW,numL,epsilon)\n",
    "        if graphics:\n",
    "            pygame.quit()\n",
    "        curgame += 1\n",
    "    if graphics:\n",
    "        sys.exit\n",
    "    return numL,numW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    numL, numW = playGame(Q,epsilonDecayFactor,learningRate,mazename,30,1,True,True,False,numghosts,gpositions,1)\n",
    "    numL, numW = playGame(Q,1,learningRate,mazename,200,1000,False,False,smartghosts,numghosts,gpositions,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printQ(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stateMoveTuple() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-100cff658845>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnumL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmazename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumGames\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msmartghosts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumghosts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgpositions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-ea7acc33655e>\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m(Q, epsilonDecayFactor, learningRate, mazename, gamespeed, numGames, debug, graphics, smartghosts, numghosts, gpositions, showRate)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0museQ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                     \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpacman\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mghosts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mpacman\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-dfd681e0b09f>\u001b[0m in \u001b[0;36mmove\u001b[1;34m(self, maze, Q, epsilon, ghosts)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mghosts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetSpaceSymbol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaze\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-b3051c80ed38>\u001b[0m in \u001b[0;36mQMove\u001b[1;34m(sprite, maze, Q, epsilon, ghosts)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mQs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstateMoveTuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msprite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mghosts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetTile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msprite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalidMoves\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mdirect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidMoves\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mstateMoveTuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msprite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mghosts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdirect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgetTile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msprite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdirect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstateMoveTuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msprite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mghosts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdirect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgetTile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msprite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdirect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstateMoveTuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msprite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mghosts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdirect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgetTile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msprite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdirect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: stateMoveTuple() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "numL, numW = playGame(Q,1,learningRate,mazename,200,numGames*5,False,False,smartghosts,numghosts,gpositions,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printQ(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numL, numW = playGame(Q,.99999,learningRate,mazename,200,numGames,False,False,smartghosts,numghosts,gpositions,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of Wins: \" + str(numW))\n",
    "print(\"Number of Losses: \" + str(numL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printQ(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numL, numW = playGame(Q,.99,learningRate,mazename,200,1000,False,False,smartghosts,numghosts,gpositions,showRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Wins: \" + str(numW))\n",
    "print(\"Number of Losses: \" + str(numL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printQ(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numL, numW = playGame(Q,epsilonDecayFactor,learningRate,mazename,gamespeed,100,False,graphics,smartghosts,numghosts,gpositions,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Wins: \" + str(numW))\n",
    "print(\"Number of Losses: \" + str(numL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numL, numW = playGame(Q,0,learningRate,mazename,gamespeed,10,False,graphics,smartghosts,numghosts,gpositions,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printQ(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
